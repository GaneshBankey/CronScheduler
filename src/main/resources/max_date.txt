import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val df = Seq(
  (1, "11-12-1980"), 
  (2, "03-05-2007"), 
  (3, "05-09-1990"), 
  (4, "09-08-2002")).toDF("my_id", "my_date")

val df_date= df.select($"my_id", to_date(unix_timestamp($"my_date", "dd-MM-yyyy").cast(TimestampType)).as("my_date"))

df_date.printSchema

var max_df = df_date.agg((max($"my_date")).as("max_date"))


max_df.show()


var filtered_df = df_date.filter($"my_date" !== max_df.first().get(0))

filtered_df.show

-----------------------------------------Another Approach------------------------------------------------------------------------------------
import java.sql.Timestamp
import java.text.SimpleDateFormat
import java.util.Date
import org.apache.spark.sql.functions.udf

def getTimestamp(x:Any) :java.sql.Timestamp = {
    val format = new SimpleDateFormat("dd-MM-yyyy")
    if (x.toString() == "") 
    return null
    else {
        val d = format.parse(x.toString());
        val t = new Timestamp(d.getTime());
        return t
    }
}

val udfGetTimestamp= udf(getTimestamp _)

val df_date= df.select($"my_id", $"my_date", udfGetTimestamp= ($"my_date").as("timestamp"))

